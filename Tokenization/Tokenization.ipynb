{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhU6F65r68B3SOqzAclXjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakarkhanlakhwera/GenAI/blob/main/Tokenization/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **White space Tokenization**"
      ],
      "metadata": {
        "id": "D7MxAlkmIUyM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7s91Dv1Hacd",
        "outputId": "6dbf69c3-d9b1-4a34-e755-c32c60e2a2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'essential', 'for', 'NLP.']\n"
          ]
        }
      ],
      "source": [
        "text = 'Tokenization is essential for NLP.'\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punctuation-Based Tokenization**"
      ],
      "metadata": {
        "id": "fxsdIn1_I_f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = 'Hello, world! Tokenization is essential for NLP.'\n",
        "tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8zVjckGI2-W",
        "outputId": "ab8e8a80-7c27-4db2-ca49-33dd6d90344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'Tokenization', 'is', 'essential', 'for', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization Using nltk**"
      ],
      "metadata": {
        "id": "V9H18_QVJem1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word tokenization**"
      ],
      "metadata": {
        "id": "C6xjdeQOJtt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = 'Tokenization is essential for NLP tokenization .'\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy6SJP7iJbA2",
        "outputId": "f7060dc3-f052-4a19-fe99-54a78dfb073c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'essential', 'for', 'NLP', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence tokenization**"
      ],
      "metadata": {
        "id": "IYIFoR-pJzY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = 'Tokenization is essential for NLP. It helps in understanding the structure of text.'\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRD8HsvJyZF",
        "outputId": "ee37a594-b319-4150-c3ba-b46444a3eaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is essential for NLP.', 'It helps in understanding the structure of text.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding of the text**"
      ],
      "metadata": {
        "id": "szf-eRbYdIna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link to data embedding globe:**https://projector.tensorflow.org/"
      ],
      "metadata": {
        "id": "6FPQydj7eCFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "6dxHMPuRKNQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Word2Vec model"
      ],
      "metadata": {
        "id": "L81zxmVqd7si"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uT61KLwrfcI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec([tokens], vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "TWTEqW4gd7KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accessing word vectors**"
      ],
      "metadata": {
        "id": "uHz9G3aXey6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['Tokenization']\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slDTyBypeArx",
        "outputId": "9e59538e-8384-4db2-87b5-2e6d5c8e1766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
            " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
            " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
            " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
            "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
            "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
            "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
            "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
            " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
            " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
            "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
            "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
            " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
            " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
            " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
            " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
            " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
            " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
            " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
            "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
            "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
            " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
            " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
            " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
            "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding similar words**"
      ],
      "metadata": {
        "id": "UCQxffbcfBr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.wv.most_similar('Tokenization')\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EWDoXfre_5R",
        "outputId": "f1c376a2-d987-4ecc-81c4-3d208f58c87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 0.1459505707025528), ('for', 0.041577354073524475), ('essential', 0.03476494178175926), ('is', 0.019152268767356873), ('.', 0.01613469421863556), ('tokenization', -0.11410722881555557)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvNXuBCagDTi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}